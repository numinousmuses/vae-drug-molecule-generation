{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install rdkit-pypi==2021.9.4","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport matplotlib.pyplot as plt\nfrom rdkit import Chem, RDLogger\nfrom rdkit.Chem.Draw import MolsToGridImage\n\nRDLogger.DisableLog(\"rdApp.*\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/zinc250k/250k_rndm_zinc_drugs_clean_3.csv\")\ndf['smiles'] = df['smiles'].apply(lambda s: s.replace('\\n', ''))\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def molecule_from_smiles(smiles):\n    # MolFromSmiles(m, sanitize=True) should be equivalent to\n    # MolFromSmiles(m, sanitize=False) -> SanitizeMol(m) -> AssignStereochemistry(m, ...)\n    molecule = Chem.MolFromSmiles(smiles, sanitize=False)\n\n    # If sanitization is unsuccessful, catch the error, and try again without\n    # the sanitization step that caused the error\n    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n        Chem.SanitizeMol(molecule, sanitizeOps=Chem.SanitizeFlags.SANITIZE_ALL ^ flag)\n\n    Chem.AssignStereochemistry(molecule, cleanIt=True, force=True)\n    return molecule","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"SMILES:\\t{df.smiles[100]}\\nlogP:\\t{df.logP[100]}\\nqed:\\t{df.qed[100]}\")\nmolecule = molecule_from_smiles(df.iloc[29].smiles)\nprint(\"Molecule:\")\nmolecule","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SMILE_CHARSET = '[\"C\", \"B\", \"F\", \"I\", \"H\", \"O\", \"N\", \"S\", \\\n                  \"P\", \"Cl\", \"Br\"]'             \nbond_mapping = {\n    \"SINGLE\": 0,\n    0: Chem.BondType.SINGLE,\n    \"DOUBLE\": 1,\n    1: Chem.BondType.DOUBLE,\n    \"TRIPLE\": 2,\n    2: Chem.BondType.TRIPLE,\n    \"AROMATIC\": 3,\n    3: Chem.BondType.AROMATIC,\n}\nSMILE_CHARSET = ast.literal_eval(SMILE_CHARSET)\n\nMAX_MOLSIZE = max(df['smiles'].str.len())\nSMILE_to_index = dict((c, i) for i, c in enumerate(SMILE_CHARSET))\nindex_to_SMILE = dict((i, c) for i, c in enumerate(SMILE_CHARSET))\natom_mapping = dict(SMILE_to_index)\natom_mapping.update(index_to_SMILE)\n\nprint(\"Max molecule size: {}\".format(MAX_MOLSIZE))\nprint(\"Character set Length: {}\".format(len(SMILE_CHARSET)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nEPOCHS =10\n\nVAE_LR = 5e-4\nNUM_ATOMS = 120 # Maximum number of atoms\n\nATOM_DIM = len(SMILE_CHARSET)  # Number of atom types\nBOND_DIM = 4 + 1  # Number of bond types\nLATENT_DIM = 435  # Size of the latent space","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smiles_to_graph(smiles):\n    '''\n    Reference: https://keras.io/examples/generative/wgan-graphs/\n    '''\n    # Converts SMILES to molecule object\n    molecule = Chem.MolFromSmiles(smiles)\n\n    # Initialize adjacency and feature tensor\n    adjacency = np.zeros((BOND_DIM, NUM_ATOMS, NUM_ATOMS), \"float32\")\n    features = np.zeros((NUM_ATOMS, ATOM_DIM), \"float32\")\n\n    # loop over each atom in molecule\n    for atom in molecule.GetAtoms():\n        i = atom.GetIdx()\n        atom_type = atom_mapping[atom.GetSymbol()]\n        features[i] = np.eye(ATOM_DIM)[atom_type]\n        # loop over one-hop neighbors\n        for neighbor in atom.GetNeighbors():\n            j = neighbor.GetIdx()\n            bond = molecule.GetBondBetweenAtoms(i, j)\n            bond_type_idx = bond_mapping[bond.GetBondType().name]\n            adjacency[bond_type_idx, [i, j], [j, i]] = 1\n    # Where no bond, add 1 to last channel (indicating \"non-bond\")\n    # Notice: channels-first\n    adjacency[-1, np.sum(adjacency, axis=0) == 0] = 1\n\n    # Where no atom, add 1 to last column (indicating \"non-atom\")\n    features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1\n\n    return adjacency, features\n\ndef graph_to_molecule(graph):\n    '''\n    Reference: https://keras.io/examples/generative/wgan-graphs/\n    '''\n    # Unpack graph\n    adjacency, features = graph\n\n    # RWMol is a molecule object intended to be edited\n    molecule = Chem.RWMol()\n\n    # Remove \"no atoms\" & atoms with no bonds\n    keep_idx = np.where(\n        (np.argmax(features, axis=1) != ATOM_DIM - 1)\n        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)\n    )[0]\n    features = features[keep_idx]\n    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n\n    # Add atoms to molecule\n    for atom_type_idx in np.argmax(features, axis=1):\n        atom = Chem.Atom(atom_mapping[atom_type_idx])\n        _ = molecule.AddAtom(atom)\n\n    # Add bonds between atoms in molecule; based on the upper triangles\n    # of the [symmetric] adjacency tensor\n    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n    for (bond_ij, atom_i, atom_j) in zip(bonds_ij, atoms_i, atoms_j):\n        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n            continue\n        bond_type = bond_mapping[bond_ij]\n        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n\n    # Sanitize the molecule; for more information on sanitization, see\n    # https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization\n    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n    # Let's be strict. If sanitization fails, return None\n    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n        return None\n\n    return molecule","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    def __init__(self, data, mapping, max_len, batch_size=6, shuffle=True):\n        \"\"\"\n        Initialization\n        \"\"\"\n        self.data = data\n        self.indices = self.data.index.tolist()\n        self.mapping = mapping\n        self.max_len = max_len\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.ceil(len(self.data) / self.batch_size))\n\n    def __getitem__(self, index):\n        if (index + 1) * self.batch_size > len(self.indices):\n            self.batch_size = len(self.indices) - index * self.batch_size\n        # Generate one batch of data\n        # Generate indices of the batch\n        index = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n        # Find list of IDs\n        batch = [self.indices[k] for k in index]\n        mol_features, mol_property = self.data_generation(batch)\n\n        return mol_features, mol_property\n\n    def on_epoch_end(self):\n\n        \"\"\"\n        Updates indexes after each epoch\n        \"\"\"\n        self.index = np.arange(len(self.indices))\n        if self.shuffle == True:\n            np.random.shuffle(self.index)\n\n    def load(self, idx):\n        \"\"\"\n        Load molecules adjacency matrix and features matrix from SMILE representation \n        and their respective SAS value.\n        \"\"\"\n        qed = self.data.loc[idx]['qed']\n\n        adjacency, features = smiles_to_graph(self.data.loc[idx]['smiles'])\n\n        return adjacency, features, qed\n\n        \n    def data_generation(self, batch):\n\n        x1 = np.empty((self.batch_size, BOND_DIM, self.max_len, self.max_len))\n        x2 = np.empty((self.batch_size, self.max_len, len(self.mapping)))\n        x3 = np.empty((self.batch_size, ))\n        \n        for i, batch_id in enumerate(batch):\n            x1[i,], x2[i,], x3[i,] = self.load(batch_id)\n\n        return [x1, x2], x3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = df.sample(frac=0.75,random_state=42) #random state is a seed value\ntest_df = df.drop(train_df.index)\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n    Reference: https://keras.io/examples/generative/wgan-graphs/\n'''\nclass RelationalGraphConvLayer(keras.layers.Layer):\n    def __init__(\n        self,\n        units=128,\n        activation=\"relu\",\n        use_bias=False,\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"zeros\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.units = units\n        self.activation = keras.activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n        self.bias_initializer = keras.initializers.get(bias_initializer)\n        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n\n    def build(self, input_shape):\n        bond_dim = input_shape[0][1]\n        atom_dim = input_shape[1][2]\n\n        self.kernel = self.add_weight(\n            shape=(bond_dim, atom_dim, self.units),\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            trainable=True,\n            name=\"W\",\n            dtype=tf.float32,\n        )\n\n        if self.use_bias:\n            self.bias = self.add_weight(\n                shape=(bond_dim, 1, self.units),\n                initializer=self.bias_initializer,\n                regularizer=self.bias_regularizer,\n                trainable=True,\n                name=\"b\",\n                dtype=tf.float32,\n            )\n        self.built = True\n\n    def call(self, inputs, training=False):\n        adjacency, features = inputs\n        # Aggregate information from neighbors\n        x = tf.matmul(adjacency, features[:, None, :, :])\n        # Apply linear transformation\n        x = tf.matmul(x, self.kernel)\n        if self.use_bias:\n            x += self.bias\n        # Reduce bond types dim\n        x_reduced = tf.reduce_sum(x, axis=1)\n        # Apply non-linear transformation\n        return self.activation(x_reduced)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_encoder(gconv_units, latent_dim, adjacency_shape, feature_shape, dense_units,dropout_rate ):\n    '''\n    Reference: https://keras.io/examples/generative/wgan-graphs/\n    '''\n    adjacency = keras.layers.Input(shape=adjacency_shape)\n    features = keras.layers.Input(shape=feature_shape)\n\n    # Propagate through one or more graph convolutional layers\n    features_transformed = features\n    for units in gconv_units:\n        features_transformed = RelationalGraphConvLayer(units)(\n            [adjacency, features_transformed]\n        )\n    # Reduce 2-D representation of molecule to 1-D\n    x = keras.layers.GlobalAveragePooling1D()(features_transformed)\n\n    # Propagate through one or more densely connected layers\n    for units in dense_units:\n        x = layers.Dense(units, activation=\"relu\")(x)\n        x = layers.Dropout(dropout_rate)(x)\n    \n    z_mean = layers.Dense(latent_dim, dtype=\"float32\", name=\"z_mean\")(x)\n    log_var = layers.Dense(latent_dim, dtype=\"float32\", name=\"log_var\")(x)\n    \n    encoder = keras.Model([adjacency, features], [z_mean, log_var], name=\"encoder\")\n    return encoder\n\ndef get_decoder(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape):\n    '''\n    Reference: https://keras.io/examples/generative/wgan-graphs/\n    '''\n    latent_inputs = keras.Input(shape=(latent_dim,))\n\n    x = latent_inputs\n    for units in dense_units:\n        x = keras.layers.Dense(units, activation=\"tanh\")(x)\n        x = keras.layers.Dropout(dropout_rate)(x)\n\n    # Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)\n    x_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(x)\n    x_adjacency = keras.layers.Reshape(adjacency_shape)(x_adjacency)\n    # Symmetrify tensors in the last two dimensions\n    x_adjacency = (x_adjacency + tf.transpose(x_adjacency, (0, 1, 3, 2))) / 2\n    x_adjacency = keras.layers.Softmax(axis=1)(x_adjacency)\n\n    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)\n    x_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(x)\n    x_features = keras.layers.Reshape(feature_shape)(x_features)\n    x_features = keras.layers.Softmax(axis=2)(x_features)\n\n\n    decoder = keras.Model(latent_inputs, outputs=[x_adjacency, x_features], name=\"decoder\")\n\n    return decoder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Sampling(layers.Layer):\n    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding.\"\"\"\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_log_var)[0]\n        dim = tf.shape(z_log_var)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MoleculeGenerator(keras.Model):\n    def __init__(self, encoder, decoder, max_len, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.property_prediction_layer = layers.Dense(1)\n        self.max_len = max_len\n\n        self.train_total_loss_tracker = keras.metrics.Mean(name=\"train_total_loss\")\n        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_total_loss\")\n\n    def train_step(self, data):\n        mol_features, mol_property = data\n        graph_real = mol_features\n        self.batch_size = tf.shape(mol_property)[0]\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, property_prediction, \\\n             reconstruction_adjacency, reconstruction_features = self(mol_features,\n                                                                             training=True)\n            graph_generated = [reconstruction_adjacency, reconstruction_features]\n            total_loss = self.calculate_loss(z_log_var,\n                                             z_mean,\n                                             mol_property,\n                                             property_prediction,\n                                             graph_real,\n                                             graph_generated,\n                                             is_train=True)\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n\n        self.train_total_loss_tracker.update_state(total_loss)\n        return {\n            \"loss\": self.train_total_loss_tracker.result(),\n        }\n\n    def test_step(self, data):\n        mol_features, mol_property = data\n        z_mean, z_log_var, property_prediction, \\\n        reconstruction_adjacency, reconstruction_features = self(mol_features,\n                                                                training=False)\n        total_loss = self.calculate_loss(z_log_var,\n                                        z_mean,\n                                        mol_property, \n                                        property_prediction,\n                                        graph_real=mol_features,\n                                        graph_generated=[reconstruction_adjacency, \n                                                         reconstruction_features],\n                                        is_train=False)\n\n        self.val_total_loss_tracker.update_state(total_loss)\n        return {\n            \"loss\": self.val_total_loss_tracker.result()\n        }\n    def calculate_loss(self,\n                       z_log_var,\n                       z_mean,\n                       mol_property,\n                       property_prediction,\n                       graph_real,\n                       graph_generated,\n                       is_train):\n        adjacency_real, features_real = graph_real\n        adjacency_generated, features_generated = graph_generated\n        \n        adjacency_reconstruction_loss = tf.reduce_mean(\n                tf.reduce_sum(\n                    keras.losses.categorical_crossentropy(\n                                                        adjacency_real,\n                                                        adjacency_generated\n                                                        ),\n                                                        axis=(1,2)\n                    )\n            )\n        features_reconstruction_loss = tf.reduce_mean(\n                tf.reduce_sum(\n                    keras.losses.categorical_crossentropy(\n                                                        features_real,\n                                                        features_generated\n                                                        ),\n                                                        axis=(1)\n                    )\n            )\n        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), 1)\n        kl_loss = tf.reduce_mean(kl_loss)\n\n        property_prediction_loss = tf.reduce_mean(\n            keras.losses.binary_crossentropy(mol_property, \n                                                property_prediction)\n        )\n\n        if is_train:\n            graph_loss = self._gradient_penalty(graph_real, graph_generated)\n        else:\n            graph_loss = 0\n\n        return kl_loss + property_prediction_loss + graph_loss + adjacency_reconstruction_loss + features_reconstruction_loss\n\n    def _gradient_penalty(self, graph_real, graph_generated):\n        # Unpack graphs\n        adjacency_real, features_real = graph_real\n        adjacency_generated, features_generated = graph_generated\n\n        # Generate interpolated graphs (adjacency_interp and features_interp)\n        alpha = tf.random.uniform([self.batch_size])\n        alpha = tf.reshape(alpha, (self.batch_size, 1, 1, 1))\n        adjacency_interp = (adjacency_real * alpha) + (1 - alpha) * adjacency_generated\n        alpha = tf.reshape(alpha, (self.batch_size, 1, 1))\n        features_interp = (features_real * alpha) + (1 - alpha) * features_generated\n\n        # Compute the logits of interpolated graphs\n        with tf.GradientTape() as tape:\n            tape.watch(adjacency_interp)\n            tape.watch(features_interp)\n            _, _, logits, _,_ = self(\n                [adjacency_interp, features_interp], training=True\n            )\n        # Compute the gradients with respect to the interpolated graphs\n        grads = tape.gradient(logits, [adjacency_interp, features_interp])\n        # Compute the gradient penalty\n        grads_adjacency_penalty = (1 - tf.norm(grads[0], axis=1)) ** 2\n        grads_features_penalty = (1 - tf.norm(grads[1], axis=2)) ** 2\n        return tf.reduce_mean(\n            tf.reduce_mean(grads_adjacency_penalty, axis=(-2, -1))\n            + tf.reduce_mean(grads_features_penalty, axis=(-1))\n        )\n    \n    def inference(self, batch_size):\n        z = tf.random.normal((batch_size, LATENT_DIM))\n        reconstruction_adjacency, reconstruction_features = model.decoder.predict(z)\n        # obtain one-hot encoded adjacency tensor\n        adjacency = tf.argmax(reconstruction_adjacency, axis=1)\n        adjacency = tf.one_hot(adjacency, depth=BOND_DIM, axis=1)\n        # Remove potential self-loops from adjacency\n        adjacency = tf.linalg.set_diag(adjacency, tf.zeros(tf.shape(adjacency)[:-1]))\n        # obtain one-hot encoded feature tensor\n        features = tf.argmax(reconstruction_features, axis=2)\n        features = tf.one_hot(features, depth=ATOM_DIM, axis=2)\n        return [\n            graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])\n            for i in range(batch_size)\n        ]\n    \n    def call(self, inputs):\n        z_mean, log_var = self.encoder(inputs)\n        z = Sampling()([z_mean, log_var])\n        reconstruction_adjacency, reconstruction_features = self.decoder(z)\n\n        property_prediction = self.property_prediction_layer(z_mean)\n\n        return z_mean, log_var, property_prediction, reconstruction_adjacency, reconstruction_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataGenerator(\n    data=train_df[:8000],\n    mapping = SMILE_to_index, \n    max_len = NUM_ATOMS,\n    batch_size=BATCH_SIZE\n)\n\nvalidation_loader = DataGenerator(\n    data=test_df[:8000],\n    mapping = SMILE_to_index, \n    max_len = NUM_ATOMS,\n    batch_size=BATCH_SIZE\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae_optimizer = tf.keras.optimizers.Adam(\n    learning_rate=VAE_LR\n)\n\nencoder = get_encoder(\n    gconv_units=[9],\n    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n    feature_shape=(NUM_ATOMS, ATOM_DIM),\n    latent_dim=LATENT_DIM,\n    dense_units=[512],\n    dropout_rate=0.0,\n)\ndecoder = get_decoder(\n    dense_units=[128, 256, 512],\n    dropout_rate=0.2,\n    latent_dim=LATENT_DIM,\n    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n    feature_shape=(NUM_ATOMS, ATOM_DIM)\n)\n\nmodel = MoleculeGenerator(encoder, \n                          decoder,\n                          MAX_MOLSIZE)\n\nmodel.compile(vae_optimizer)\nhistory = model.fit(train_loader,\n                    epochs=EPOCHS,\n                    validation_data=validation_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('VAE Loss')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"molecules = model.inference(1000)\n\nMolsToGridImage(\n    [m for m in molecules if m is not None][:1000], molsPerRow=5, subImgSize=(260, 160)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_label_clusters(vae, data, labels):\n    # display a 2D plot of the property in the latent space\n    z_mean, _ = vae.encoder.predict(data)\n    plt.figure(figsize=(12, 10))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.show()\n\n\ntrain_data = DataGenerator(\n                            data=train_df,\n                            mapping = SMILE_to_index, \n                            max_len = NUM_ATOMS,\n                            batch_size=8000\n                        )\nx_train, y_train = next(iter(train_data))\nplot_label_clusters(model, x_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}